{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\" and remove every line containing the expression: \"raise ...\" (if you leave such a line your code will not run).\n",
    "\n",
    "Do not remove any cell from the notebook you downloaded. You can add any number of cells (and remove them if not more necessary).\n",
    "\n",
    "Do not leave any variable initialized to None.\n",
    "\n",
    "## IMPORTANT: make sure to rerun all the code from the beginning to obtain the results for the final version of your notebook, since this is the way we will do it before evaluating your notebook!!!\n",
    "\n",
    "## Make sure to name your notebook file (.ipynb) correctly:\n",
    "### - NL_NAMESURNAME_ID (E.g. : NL_MARIOROSSI_2204567)\n",
    "\n",
    "## Fill in your name, surname and id number (numero matricola) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Victor Miguel Velazquez Espitia\"\n",
    "ID_number = int(\"2043179\")\n",
    "\n",
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ff9eaae2f9b2bbc8db537ec711deafa",
     "grade": false,
     "grade_id": "cell-076a38b69f1df826",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## HOMEWORK #3\n",
    "\n",
    "### Non linear models for classification \n",
    "\n",
    "In this notebook we are going to explore the use of SVM and Neural Networks for image classification. We are going to use the famous MNIST dataset, that is a dataset of handwritten digits. We get the data from mldata.org, that is a public repository for machine learning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9bbaec1fea909334962e7a4c356aeba9",
     "grade": false,
     "grade_id": "cell-4ba73b30a541fdaa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the required packages\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "np.random.seed(ID_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71a63b13bc065b9c4b1b3353a137ec65",
     "grade": false,
     "grade_id": "cell-87d195f3039741b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#load the MNIST dataset \n",
    "#Load data from https://www.openml.org/d/554\n",
    "X,Y = fetch_openml('mnist_784', version=1, return_X_y=True,as_frame = False)\n",
    "\n",
    "print(f'Each image is represented as vector of shape {X[0].shape}')\n",
    "print(f'The image is represented in gray scale levels {X[0]}')\n",
    "print(f'Here it is a label: {Y[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d2074c4e63130c4fdcff05ad88933d8",
     "grade": false,
     "grade_id": "cell-8f5da37aaf5a57e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#let's normalize the features so that each value is between [0,1]\n",
    "\n",
    "# Rescale the data\n",
    "X = X / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "141a09fab5765f725c599c3565c9adaf",
     "grade": false,
     "grade_id": "cell-81c18ee97323d3e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In a classification problem it is desirable to split the dataset into train and test sets in a way that preserves the same proportions of examples in each class as observed in the original dataset.\n",
    "We can achieve this by setting the “stratify” argument of the function \"train_test_split\" to the Y component of our dataset.\n",
    "\n",
    "We are going to use 500 samples in the train dataset, the remaining ones are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c2343a80405bf126ca8caee4c8948ea",
     "grade": false,
     "grade_id": "cell-e27bcaacee1db729",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "m_t = 500\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=m_t/len(Y), random_state=ID_number, stratify=Y)\n",
    "\n",
    "print(f'Lenght train dataset: {len(y_train)}, Labels and frequencies: \\n {list(zip(*np.unique(y_train, return_counts=True)))}')\n",
    "print(f'Lenght test dataset: {len(y_test)}, Labels and frequencies: \\n {list(zip(*np.unique(y_test, return_counts=True)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "43de9c7aa4a5c89c64f8329c785e1e25",
     "grade": false,
     "grade_id": "cell-c3f9a9f2f617be66",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Function to plot a digit and print the corresponding label\n",
    "def plot_digit(X_matrix, labels, index):\n",
    "    print(\"INPUT:\")\n",
    "    plt.imshow(\n",
    "        X_matrix[index].reshape(28,28),\n",
    "        cmap          = plt.cm.gray_r,\n",
    "        interpolation = \"nearest\"\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"LABEL: {labels[index]}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f931c5ab0576c91a41e6ab50bd31881d",
     "grade": false,
     "grade_id": "cell-b2638b70d6ee2365",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#let's try the plotting function\n",
    "plot_digit(x_train, y_train, 100)\n",
    "plot_digit(x_test, y_test, 40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1216eecf5a2a2a9116c547a4bc44e292",
     "grade": false,
     "grade_id": "cell-8173b3832936e560",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TO DO 1\n",
    "SVM with cross validation to pick the best model. Use SVC from sklearn.svm and GridSearchCV from sklearn.model_selection (5-fold cross-validation).\n",
    "\n",
    "Print the best parameters found as well as the best score obtained by the 'optimal' model.\n",
    "Choose the grid, depending on the kernel you are using different hyper-parameters are needed (C, gamma, ...). \n",
    "You do not need to use more than 5 values for each hyper-parameter (otherwise the cell could be very slow). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ceb75594dc13c0f7d19fdea2d17a2319",
     "grade": false,
     "grade_id": "cell-a5339e3c792f1d3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#import SVC\n",
    "from sklearn.svm import SVC\n",
    "#import for Cross-Validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def compute_best_SVM_with_CV(kernel_type : str, parameters : dict, x_train : np.ndarray, y_train : np.ndarray) -> tuple:\n",
    "    '''\n",
    "    Use Cross validation to find the best SVM on the given parameters. Return the best parameters set together with \n",
    "    the corresponding score. Return also the scores for all the other parameters given as input.\n",
    "    :param kernel_type: Type of kernel (i.e. linear, rbf, poly)\n",
    "    :param parameters: Dict containing kernel parameters (e.g. {'C': [1, 10, 100, 1000], 'gamma': [0.01, 0.001], ...})\n",
    "    :param x_train: Train dataset\n",
    "    :param y_train: Train labels\n",
    "    \n",
    "    :returns: (best_param, best_score, all_scores)\n",
    "        WHERE:\n",
    "        best_param: best parameter set (this is a dictionary)\n",
    "        best_score: best score obtained for the given parameters (float)\n",
    "        all_scores: all scores computed for each parameter (np.ndarray)\n",
    "    '''\n",
    "    SVM_model = SVC(kernel=kernel_type)\n",
    "    # Use GridSearchCV to find the best parameter set.\n",
    "    # YOUR CODE HERE\n",
    "    cv = GridSearchCV(SVM_model, parameters)\n",
    "    cv.fit(x_train,  y_train)\n",
    "    #raise NotImplementedError() # Remove this line\n",
    "    \n",
    "    print('#####################################')\n",
    "    print(f'RESULTS for {kernel_type} KERNEL\\n')\n",
    "    # Store the best parameters set and print them\n",
    "    print(\"Best parameters set found:\")\n",
    "    best_param = None\n",
    "    # YOUR CODE HERE\n",
    "    best_param = cv.best_params_\n",
    "    #raise NotImplementedError() # Remove this line\n",
    "    print(best_param)\n",
    "    \n",
    "    # Store and print the score of the best parameters set\n",
    "    print(\"\\nScore with best parameters:\")\n",
    "    best_score = None\n",
    "    # YOUR CODE HERE\n",
    "    best_score= cv.best_score_\n",
    "    #raise NotImplementedError() # Remove this line\n",
    "    print(best_score)\n",
    "    \n",
    "    # Store and print all the scores for the given parameters (average of the validation scores)\n",
    "    print(\"\\nAll scores on the grid:\")\n",
    "    all_scores = None\n",
    "    # YOUR CODE HERE\n",
    "    all_scores= cv.cv_results_['mean_test_score']\n",
    "    #raise NotImplementedError() # Remove this line\n",
    "    print(all_scores)\n",
    "    \n",
    "    return best_param, best_score, all_scores\n",
    "\n",
    "# Choose the grid for parameters of the linear SVM kernel\n",
    "linear_parameters = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "linear_parameters = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "#raise NotImplementedError() # Remove this line\n",
    "best_param_lin, best_score_lin, all_scores_lin = compute_best_SVM_with_CV('linear', linear_parameters, x_train, y_train)\n",
    "# Choose the grid for parameters of the rbf SVM kernel\n",
    "rbf_parameters = None\n",
    "# YOUR CODE HERE\n",
    "rbf_parameters={'C': [1, 10, 100, 1000] , 'gamma': [0.01, 0.001] }\n",
    "best_param_rbf, best_score_rbf, all_scores_rbf = compute_best_SVM_with_CV('rbf', rbf_parameters, x_train, y_train)\n",
    "# Choose the grid for parameters of the poly SVM kernel (do not forget to choose the degree)\n",
    "poly_parameters = None\n",
    "# YOUR CODE HERE\n",
    "poly_parameters ={'C': [1, 10, 100, 1000] , 'degree':[2,3,4] }\n",
    "#raise NotImplementedError() # Remove this line\n",
    "best_param_poly, best_score_poly, all_scores_poly = compute_best_SVM_with_CV('poly', poly_parameters, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "207aeacb91c04b8b825a669594ddae01",
     "grade": true,
     "grade_id": "cell-ebaae7b6afc0d0cf",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(best_param_rbf) == dict\n",
    "assert type(best_score_rbf) == np.float64\n",
    "assert np.prod(np.array([len(params) for params in rbf_parameters.values()])) == len(all_scores_rbf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b144521663d002b96ffe699cda280b19",
     "grade": false,
     "grade_id": "cell-afe8e1dfdc2607ec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO 2: \n",
    "# Get training and test error for the best SVM model obtained from CV (you need to choose across different kernels \n",
    "# too). You just need to look at the best model for each kernel and choose the best one (you can do this by hand).\n",
    "\n",
    "# YOUR CODE HERE\n",
    "best_kernel_type, best_parameters = 'rbf' , {'C': 10, 'gamma': 0.01}\n",
    "\n",
    "best_SVM = SVC(kernel=best_kernel_type, **best_parameters)\n",
    "best_SVM.fit(x_train, y_train)\n",
    "\n",
    "# Compute training and test error for this model (use the usual sklearn built-in functions)\n",
    "training_error, test_error = 1-best_SVM.score(x_train,y_train) , 1-best_SVM.score(x_test,y_test)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print (f\"Best SVM training error: {training_error}\")\n",
    "print (f\"Best SVM test error: {test_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57d9c76ee5722becfaa9cf0bbc261d16",
     "grade": true,
     "grade_id": "cell-4053a73d188e1e77",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(training_error) == np.float64\n",
    "assert type(test_error) == np.float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "848f73ea4a4b9321f882ef7436d95f7b",
     "grade": false,
     "grade_id": "cell-725922e6187a3cca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TO DO 3\n",
    "Now we use feed-forward neural networks for classification. \n",
    "In particular, we use the Multi-Layer-Perceptron (the multi-layer structure we have seen in class, see http://scikit-learn.org/stable/modules/neural_networks_supervised.html).\n",
    "\n",
    "Similarly as before, we use cross validation to pick the best model, you need to complete the function 'compute_best_MLP_with_CV()' that finds the best MLP architecture given a specific activation function.\n",
    "\n",
    "Note that the starting random state is fixed to make the runs reproducible (random_state=ID_number).\n",
    "The following options for the MLP are used: max_iter=1000, alpha=1e-4, solver='sgd', tol=1e-4, random_state=ID_number, learning_rate_init=.1, activation = activation_f. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "473bad63d44a54a6214f207e64471b66",
     "grade": false,
     "grade_id": "cell-c361dd7fdf8988a7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_best_MLP_with_CV(activation_f : str, parameters : dict, x_train : np.ndarray, y_train : np.ndarray) -> tuple:\n",
    "    '''\n",
    "    Use Cross validation to find the best MLP architecture given a specific activation function. \n",
    "    Return the best parameters set together with the corresponding score. Return also the scores for all the other parameters given as input.\n",
    "    :param activation_f: Type of activation function (e.g. 'logistic', 'tanh', 'relu')\n",
    "    :param parameters: architectures (e.g. {'hidden_layer_sizes': [(10,), (50,), (10,10,), (50,50,)]})\n",
    "    :param x_train: Train dataset\n",
    "    :param y_train: Train labels\n",
    "    \n",
    "    :returns: (best_param, best_score, all_scores)\n",
    "        WHERE:\n",
    "        best_param: best parameter set (this is a dictionary)\n",
    "        best_score: best score obtained for the given parameters (float)\n",
    "        all_scores: all scores computed for each parameter (np.ndarray)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    mlp = MLPClassifier(max_iter=1000, alpha=1e-4, solver='sgd', tol=1e-4, random_state=ID_number, learning_rate_init=.1,activation = activation_f)\n",
    "    \n",
    "    #Use GridSearchCV to find the various paramters the function returns: best_param, best_score, all_scores\n",
    "    mlp_CV = GridSearchCV(mlp, parameters)\n",
    "    mlp_CV.fit(x_train,  y_train)\n",
    "\n",
    "    best_param = mlp_CV.best_params_\n",
    "    best_score= mlp_CV.best_score_\n",
    "    all_scores= mlp_CV.cv_results_['mean_test_score']\n",
    "\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    return best_param, best_score, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64584929084e1095ece426b1dd796a8f",
     "grade": false,
     "grade_id": "cell-ed0e8862239bb750",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#test various architectures (hidden_layer_sizes) and activation functions (e.g. 'logistic','tanh','relu') for the MLP.\n",
    "\n",
    "mlp_parameters = parameters = {'hidden_layer_sizes': [(10,), (50,), (10,10,), (50,50,)]}\n",
    " #leave here maximum 3 architectures when you submit\n",
    "\n",
    "# next test different architectures and activation functions: use compute_best_MLP_with_CV()\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a148d3c7fadf45f4ce4de70c33d14e0",
     "grade": true,
     "grade_id": "cell-5a81be296062b879",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#simple autotest with relu\n",
    "best_param_relu, best_score_relu, all_scores_relu = compute_best_MLP_with_CV('relu', mlp_parameters, x_train, y_train)\n",
    "\n",
    "assert type(best_param_relu) == dict\n",
    "assert type(best_score_relu) == np.float64\n",
    "assert np.prod(np.array([len(params) for params in mlp_parameters.values()])) == len(all_scores_relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae813325d9c0f004614b39c912d1f4b5",
     "grade": false,
     "grade_id": "cell-41c31d1b09937db6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Select the best activation function and architecture you found so that it can be used next\n",
    "\n",
    "best_activation_type, mlp_best_param = 'relu', mlp_parameters\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "891f23035dcc696a89f5201508b436b0",
     "grade": false,
     "grade_id": "cell-3b3105ef206a32a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TO DO 4\n",
    "\n",
    "\n",
    "Now get training and test error for the NN with the best parameters from above. We use verbose=True\n",
    "in input so to see how loss changes in iterations (see how this changes if the number of iterations is changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f20c5b09c0f6d05abb8a73f4965fa347",
     "grade": false,
     "grade_id": "cell-1bac118c0979b347",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get training and test error for the best NN model found using CV\n",
    "max_iter = 1000\n",
    "mlp = MLPClassifier(**mlp_best_param, max_iter=max_iter, alpha=1e-4, solver='sgd', tol=1e-4, random_state=ID_number,\n",
    "                    learning_rate_init=.1,activation=best_activation_type, verbose=True)\n",
    "\n",
    "# ADD CODE: FIT MODEL & COMPUTE TRAINING AND TEST ERRORS\n",
    "mlp.fit(x_train,  y_train)\n",
    "training_error, test_error = 1-mlp.score(x_train,y_train) , 1-mlp.score(x_test,y_test)# YOUR CODE HERE\n",
    "\n",
    "print ('\\nRESULTS FOR BEST NN\\n')\n",
    "\n",
    "print (\"Best NN training error: %f\" % training_error)\n",
    "print (\"Best NN test error: %f\" % test_error)\n",
    "\n",
    "plt.plot(mlp.loss_curve_, label='Training Loss')\n",
    "plt.title('Training loss MLP')\n",
    "plt.xlabel('Iter'), plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd264ef221a9a75f3e41dfb40deff87c",
     "grade": true,
     "grade_id": "cell-b01aa80f3128b91d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(training_error) == np.float64\n",
    "assert type(test_error) == np.float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e8c8979eadb59da78e051f520daab77a",
     "grade": false,
     "grade_id": "cell-53fd0895cc28155b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TO DO  5\n",
    "Write a function to find and plot the first digit (in x_test) that is missclassified by NN and correctly classified by SVM.\n",
    "\n",
    "Write a function to compute the confusion matrix for the predictions of a model (on testset). If you are not familiar with what a confusion matrix is, have a look at this link: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html . You are not allowed to use sklearn to create the confusion matrix BUT you can compare your solution with the sklearn implementation to check you wrote it right (see assert checks). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fc9dab81c17160be561fcf3e5219204",
     "grade": false,
     "grade_id": "cell-6abbf994e14cb54d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_and_print_first_mismatched_prediction(SVM_prediction : np.ndarray, NN_prediction : np.ndarray,\n",
    "                                               x_test : np.ndarray, y_test : np.ndarray) -> int:\n",
    "    '''\n",
    "    Function to find and print the first digit that is missclassified by NN and correctly classified by SVM.\n",
    "    :param SVM_prediction: SVM predicitons.\n",
    "    :param NN_prediction: MLP predicitons.\n",
    "    :param x_test: Test set inputs.\n",
    "    :param y_test: Test set labels.\n",
    "    \n",
    "    :returns:\n",
    "        i: returns the first index in which there is a mismatch between NN_prediction and true labels but no mismatch \n",
    "           between SVM_prediction and true labels. \n",
    "    '''\n",
    "    i = 0\n",
    "    found = False\n",
    "    while ((not found) and (i<len(y_test))):\n",
    "        # YOUR CODE HERE\n",
    "        if (y_test[i] == SVM_prediction[i] and y_test[i] != NN_prediction[i]):\n",
    "            found = True\n",
    "        else:\n",
    "            i = i+1\n",
    "    return i\n",
    "    \n",
    "    \n",
    "def confusion_matrix_by_hand(true_labels : np.ndarray, predicted_labels : np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Function used to compute the confusion matrix given true and predicted labels. \n",
    "    :param true_labels: True labels.\n",
    "    :param predicted_labels: Predicted labels (note this function does not require to know which model generated \n",
    "                             the predictions).\n",
    "    \n",
    "    :returns:\n",
    "        confusion_matrix: Confusion matrix for the given true and predicted labels.\n",
    "    '''\n",
    "    labels = np.unique(true_labels)\n",
    "    map_labels_to_index = {label:i for i, label in enumerate(labels)}\n",
    "    confusion_matrix = np.zeros((len(labels), len(labels)))\n",
    "    # YOUR CODE HERE\n",
    "    for p,a in zip(true_labels, predicted_labels):\n",
    "        confusion_matrix[ map_labels_to_index[p]][ map_labels_to_index[a]] += 1\n",
    "    return confusion_matrix.astype(int)\n",
    "#predicted & true labels\n",
    "\n",
    "# Let's test our functions\n",
    "SVM_prediction = best_SVM.predict(x_test)\n",
    "NN_prediction = mlp.predict(x_test)\n",
    "\n",
    "\n",
    "first_index = find_and_print_first_mismatched_prediction(SVM_prediction, NN_prediction, x_test, y_test)\n",
    "\n",
    "SVM_CM = confusion_matrix_by_hand(y_test, SVM_prediction)\n",
    "MLP_CM = confusion_matrix_by_hand(y_test, NN_prediction)\n",
    "\n",
    "print(f'SVM confusion matrix: {SVM_CM}')\n",
    "print(f'MLP confusion matrix: {MLP_CM}')\n",
    "\n",
    "# Convert confusion matrices to pandas data frames\n",
    "labels = np.unique(y_test)\n",
    "SVM_CM_df = pd.DataFrame(SVM_CM, index = labels, columns = labels)\n",
    "MLP_CM_df = pd.DataFrame(MLP_CM, index = labels, columns = labels)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "sn.heatmap(SVM_CM_df, annot=True, ax=axes[0], cmap='rocket_r', vmax=450)\n",
    "sn.heatmap(MLP_CM_df, annot=True, ax=axes[1], cmap='rocket_r', vmax=450)\n",
    "axes[0].set_title('SVM'), axes[1].set_title('MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "581fbd42241ce18d49f068d6e20787db",
     "grade": true,
     "grade_id": "cell-6f56cf18ba3807e3",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "skl_confusion_matrix_SVM = confusion_matrix(y_test, SVM_prediction)\n",
    "skl_confusion_matrix_NN = confusion_matrix(y_test, NN_prediction)\n",
    "\n",
    "assert np.sum(skl_confusion_matrix_SVM - SVM_CM) == 0\n",
    "assert np.sum(skl_confusion_matrix_NN - MLP_CM) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb4f3eb9ac1a3128b1d7935e6214a2b3",
     "grade": false,
     "grade_id": "cell-e43e0672f19b397b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TO DO 6: explain the results you got (max 5 lines)\n",
    "According to the cross-validation results, would you choose SVMs or NNs when 500 data points are available for training? Is this a good choice, given the results on the test set?\n",
    "\n",
    "Looking at the confusion matrices what to do you observe? On which classes each model is more likely to make mistakes? \n",
    "\n",
    "(Answer in the next cell, no need to add code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9449a004af26a5f1cee1b3e73dd770b",
     "grade": true,
     "grade_id": "cell-2169ccc88e433acd",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Observing the values of the error on the test set for best SVM (the one using rbf kernel function, as we expect from the theory because the rbf kernel's shape is similar to a gaussian) and best NN (with 50/50) it is possible to notice that SVM creates a better model because the test error for SVM (0.108) is less than that of NN (0.1467). This is what we expect from the theory: while NN finds a generale hyperplane that separates the datapoints, SVM finds the one that is at the maximum distance from the points of the different subsets. The confusion matrices confirm that SVM creates a better model than NN since the values on the diagonal of the matrices, which corresponds to the number of correctly classified data, is greater for SVM than for NN. Furthermore, observing the confusion matricies it is possible to notice what are the classes where each model makes more mistakes: both SVM and NN confuse the number 4 with 9, 7 with 9, and 3 with 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c25fbb7d32aa457160151bac43dbc9c0",
     "grade": false,
     "grade_id": "cell-fee6298e8b853f42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## More Data\n",
    "\n",
    "Now let's do the same but using more data points for training SVM and NN. For SVM we are going to use the best hyperparameters set (kernel, C, gamma, ...) found using 500 data points. For NN we are going to use the best architecture found using 500 data points for the relu kernel since such architecture is usually fast to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aeed066e86503e0951b48f996040232e",
     "grade": false,
     "grade_id": "cell-a848ab7cf8710da9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#let restart the random generator with the given seed\n",
    "np.random.seed(ID_number)\n",
    "\n",
    "m_t = 60000\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=m_t/len(Y), random_state=ID_number, stratify=Y)\n",
    "\n",
    "print(f'Lenght train dataset: {len(y_train)}, Labels and frequencies: \\n {list(zip(*np.unique(y_train, return_counts=True)))}')\n",
    "print(f'Lenght test dataset: {len(y_test)}, Labels and frequencies: \\n {list(zip(*np.unique(y_test, return_counts=True)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4b6b7529768cedc77543913fab7bb84",
     "grade": false,
     "grade_id": "cell-8749fd4957d4de78",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# As we did with the first HW let's use a decorator to measure time \n",
    "from collections import defaultdict\n",
    "running_times = defaultdict(list)\n",
    "\n",
    "def measure_time(function):\n",
    "    def wrap(*args, **kw):\n",
    "        import time \n",
    "        t_start = time.time()\n",
    "        result = function(*args, **kw)\n",
    "        t_end = time.time()\n",
    "        running_times[type(args[0]).__name__].append(t_end - t_start)\n",
    "        return result\n",
    "    return wrap\n",
    "\n",
    "@measure_time\n",
    "def fit_classification_model(model, x_train, y_train):\n",
    "    model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f6c7d0fb6ef2afe2e93626ce1ce30b5",
     "grade": false,
     "grade_id": "cell-67568ad39b4b86e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "n_data = [250, 500, 1000, 2000, 5000, 7500]\n",
    "svm_train_err, svm_test_err = [], [] \n",
    "mlp_train_err, mlp_test_err = [], [] \n",
    "for n in n_data: \n",
    "    print(f'Processing with {n} data ...')\n",
    "    # Initialize models according to the best we got using 500 data\n",
    "    svm = SVC(kernel=best_kernel_type, **best_parameters)\n",
    "    mlp = MLPClassifier(**best_param_relu, max_iter=max_iter, alpha=1e-4, solver='sgd', tol=1e-4, \n",
    "                        random_state=ID_number, learning_rate_init=.1,activation='relu')\n",
    "\n",
    "    # fit svc\n",
    "    fit_classification_model(svm, x_train[:n], y_train[:n])\n",
    "    # get svc train and test error\n",
    "    svm_train_err.append(1. - svm.score(x_train[:n], y_train[:n]))\n",
    "    svm_test_err.append(1. - svm.score(x_test, y_test))\n",
    "    \n",
    "    # fit mlp\n",
    "    fit_classification_model(mlp, x_train[:n], y_train[:n])\n",
    "    # get mlp train and test error\n",
    "    mlp_train_err.append(1. - mlp.score(x_train[:n], y_train[:n]))\n",
    "    mlp_test_err.append(1. - mlp.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76cdcc33de26eab0389241936928eff6",
     "grade": false,
     "grade_id": "cell-32afc1730a114434",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(15, 5))\n",
    "axes[0].plot(n_data, np.array(svm_train_err), label='SVM train err')\n",
    "axes[0].plot(n_data, np.array(svm_test_err), label='SVM test err')\n",
    "axes[0].plot(n_data, np.array(mlp_train_err), label='MLP train err')\n",
    "axes[0].plot(n_data, np.array(mlp_test_err), label='MLP test err')\n",
    "axes[0].set_xlabel('N data'), axes[0].set_ylabel('Loss')\n",
    "axes[0].legend(), axes[0].set_title('SVC vs MLP Errors')\n",
    "\n",
    "for model, times in running_times.items():\n",
    "    axes[1].plot(n_data, times, label=model)\n",
    "axes[1].set_xlabel('N data'), axes[1].set_ylabel('Time (s)')\n",
    "axes[1].legend(), axes[1].set_title('Training Time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2cdd9b28875b481b1861df52f3f424b0",
     "grade": false,
     "grade_id": "cell-850f1836c9d53145",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TODO 7: Complete dataset\n",
    "Just for comparison, since it may not be possible to learn a SVM on too many data (due to time and memory complexity issues as you can notice from the plots above), let's use logistic regression (with standard parameters from scikit-learn but the number of iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "859390c23f4ce4fc6a5fc409a9ea5077",
     "grade": false,
     "grade_id": "cell-916c97aa3e1b4655",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Fit and test a logistic regression model\n",
    "max_iter = 1000\n",
    "\n",
    "# YOUR CODE HERE\n",
    "log_reg = linear_model.LogisticRegression(max_iter = max_iter)\n",
    "log_reg.fit(x_train, np.ravel(y_train))\n",
    "training_error_lr = 1 - log_reg.score(x_train, y_train)\n",
    "test_error_lr = 1 - log_reg.score(x_test, y_test)\n",
    "\n",
    "print (f\"Best logistic regression training error: {training_error_lr:.4f}\")\n",
    "print (f\"Best logistic regression test error: {test_error_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0a20eac6a2870207c1ad18062737b01",
     "grade": false,
     "grade_id": "cell-005fbf84d90498d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We now learn the NN. Below we use the same best architecture as before (found with 500 data for the relu activation function), feel free to try larger ones (and to use again CV), or smaller ones if it takes too much time. (We suggest that you use 'verbose=True' so have an idea of how long it takes to run 1 iteration). \n",
    "\n",
    "*Note*: If you do again CV to choose the best architecture remember to save the best set of parameters into the variable: \"best_param_relu\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07272b840428d02e8d707f2827805efd",
     "grade": false,
     "grade_id": "cell-21ceb4a0710c5e96",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#get training and test error for the best NN model from CV\n",
    "\n",
    "# YOUR CODE HERE\n",
    "parameters = {'hidden_layer_sizes': [(10,), (50,), (10,10,), (50,50,)]}\n",
    "mlp = MLPClassifier(max_iter=1000, alpha=1e-4, solver='sgd', tol=1e-4, random_state=ID_number, learning_rate_init=.1)\n",
    "mlp_CV = GridSearchCV(mlp, parameters)\n",
    "mlp_CV.fit(x_train,  y_train)\n",
    "mlp_best_param = mlp_CV.best_params_\n",
    "max_iter = 1000\n",
    "best_mlp_larger = MLPClassifier(**mlp_best_param, max_iter=max_iter, alpha=1e-4, solver='sgd', tol=1e-4, random_state=ID_number,\n",
    "                    learning_rate_init=.1, verbose=True)\n",
    "best_mlp_larger.fit(x_train,  y_train)\n",
    "training_error, test_error = 1-best_mlp_larger.score(x_train,y_train) , 1-best_mlp_larger.score(x_test,y_test)\n",
    "\n",
    "print ('\\nRESULTS FOR BEST NN\\n')\n",
    "\n",
    "print (f\"Best NN training error: {training_error:.4f}\")\n",
    "print (f\"Best NN test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92f74c6f90331e4be9a40075441dde9a",
     "grade": true,
     "grade_id": "cell-749912093733565e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(training_error) == np.float64\n",
    "assert type(test_error) == np.float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1376177ee8dfaef53f642f2e3ca2ca50",
     "grade": false,
     "grade_id": "cell-ef2ebd626161b3c3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## TODO 8: compute the confusion matrices both on train and test set for Logistic regression (trained on 60k)\n",
    "# and MLP (trained on 60k).\n",
    "\n",
    "# Log Reg Confusion matrices\n",
    "log_reg_CM_train, log_reg_CM_test = None, None\n",
    "# YOUR CODE HERE\n",
    "predict_train = log_reg.predict(x_train)\n",
    "predict_test = log_reg.predict(x_test)\n",
    "log_reg_CM_train = confusion_matrix_by_hand(y_train, predict_train)\n",
    "log_reg_CM_test = confusion_matrix_by_hand(y_test, predict_test)\n",
    "# mlp\n",
    "mlp_CM_train, mlp_CM_test = None, None\n",
    "# YOUR CODE HERE\n",
    "predict_train_mlp = best_mlp_larger.predict(x_train)\n",
    "predict_test_mlp = best_mlp_larger.predict(x_test)\n",
    "mlp_CM_train = confusion_matrix_by_hand(y_train, predict_train_mlp)\n",
    "mlp_CM_test = confusion_matrix_by_hand(y_test, predict_test_mlp)\n",
    "\n",
    "\n",
    "# Convert confusion matrices to pandas data frames\n",
    "labels = np.unique(y_test)\n",
    "log_reg_CM_train_df = pd.DataFrame(log_reg_CM_train, index = labels, columns = labels)\n",
    "log_reg_CM_test_df = pd.DataFrame(log_reg_CM_test, index = labels, columns = labels)\n",
    "\n",
    "mlp_CM_train_df = pd.DataFrame(mlp_CM_train, index = labels, columns = labels)\n",
    "mlp_CM_test_df = pd.DataFrame(mlp_CM_test, index = labels, columns = labels)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "sn.heatmap(log_reg_CM_train_df, annot=True, ax=axes[0], cmap='rocket_r', vmax=250)\n",
    "sn.heatmap(log_reg_CM_test_df, annot=True, ax=axes[1], cmap='rocket_r', vmax=250)\n",
    "axes[0].set_title('Log Reg Train'), axes[1].set_title('Log Reg Test')\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(15,5))\n",
    "sn.heatmap(mlp_CM_train_df, annot=True, ax=axes[0], cmap='rocket_r', vmax=50)\n",
    "sn.heatmap(mlp_CM_test_df, annot=True, ax=axes[1], cmap='rocket_r', vmax=50)\n",
    "axes[0].set_title('MLP Train'), axes[1].set_title('MLP Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56f669c88be1984b1e67a92933699740",
     "grade": true,
     "grade_id": "cell-20e1f551d47d4809",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert log_reg_CM_train.shape == (10, 10)\n",
    "assert log_reg_CM_test.shape == (10, 10)\n",
    "assert mlp_CM_train.shape == (10, 10)\n",
    "assert mlp_CM_test.shape == (10, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0cd0d6edc2b504fa308234016e395ed",
     "grade": false,
     "grade_id": "cell-bb8ab6d807b36a9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TO DO 9\n",
    "Compare and discuss:\n",
    "- compare the computational time required to fit a SVM and a MLP. Which is faster as the number of data increase? Why? Can you apply both methods in the high data regime?\n",
    "- the results from SVM m=7500 and NN with m=60000 training data points.\n",
    "- the results from NN with m=500 and m=60000 training data points.\n",
    "- What do you observe in the confusion matrices? Which are the hardest classes? Are the hardest and easiest classes the same both for mlp and logistic regression?\n",
    "\n",
    "(Answer in the next cell, no need to write code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac9506d7b0d0287fab6d1500ddf854a6",
     "grade": true,
     "grade_id": "cell-fd40eed840f9e7a0",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "(1)Every time the code is run, a different computational time graph is obtained because the time depends on the speed of the processor used at that moment. In any case it can be observed that the computational time has an increasing trend both for SVM and for NN. However, SVM has a better computational time than NN if the number of data is small: in fact, it solves a convex optimization problem for which there exist some tools to solve it faster, on the other hand it is a quadric problem, therefore, if the number of date is very larger it is very difficult to be solved so, in this second case, NN has a better computational time as it is possibile to see in the graphs. To concluse, if the number of data is small is more convinient to use SVM, otherwise, if the number of data is big, NN result more convinient.\n",
    "(2)The test error for NN with 60000 data(0.0268) is slightly better the the test error for SVM with 7500 data (0.0411), but it is important to notice that the time occured to determine the first model is much bigger that the time used by SVM with 7500. Therefore, despite NN with 60000 data determines a better model it is more conviniet to use SVM with 7500 data.\n",
    "(3)As we expect from the theory, NN with 60000 training data creates a better model than NN with 500 data since the error for the test set int the first case(0.0268) is lower than the one for the second case(0.146849), because the method is able to create a better model using more data.\n",
    "(4)Observing the confusion matricies it is possible to notice that MLP creates a better model than logistic regression since it creates a model that fit perfectly the data from the training set (no class is confused with another as you can see from the associated confusion matrix), while logistic regression makes some mistakes. This is what we expect from the theory: Logistic regression is able to do only linear classification while MLP method works also for non linear classification problems. If we look at the confusion matrices of both methods applied at the test set, we observe that with logistic regression more classes are confused each other than with MLP. In particular, logistic regression confuses the number 3 with 5, 8 with 5 and 9 with 7, while MLP confuses the number 3 with 5, 8 with 9, and 9 with 7. Forthumore it is possible to observe that while with 500 data points the most confused classes for MLP were 7 with 9, now using 60000 data MLP commits less error on these classes and they are not the ones associated with the grater number of mistakes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c69b5baf5767d1cd1c0f771ce3afa2d",
     "grade": false,
     "grade_id": "cell-b6b8757aa45ac7db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data normalization\n",
    "\n",
    "In the following the importance of data normalization before investigated. In particular, a MLP with a (50,50,) architecture and a 'logistic' activation function is trained with the original MNIST data and the effects are analized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f7cdd6b84a9c9124cb288f4da37aef0",
     "grade": false,
     "grade_id": "cell-51b11e60a9629efe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# data are restored to their original scale \n",
    "X = X*255.\n",
    "print(X[1])\n",
    "\n",
    "#train-test data split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=m_t/len(Y), random_state=ID_number, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e9e5b7bef253337edf9b437a21ffd0ee",
     "grade": false,
     "grade_id": "cell-2f35cc1412a6b8ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "best_mlp_large = MLPClassifier(hidden_layer_sizes=(50,50,), max_iter=max_iter, alpha=1e-4,activation='logistic', solver='sgd', tol=1e-4, \n",
    "                               random_state=None, learning_rate_init=.1, verbose=True)\n",
    "best_mlp_large.fit(x_train, y_train)\n",
    "training_error = 1. - best_mlp_large.score(x_train, y_train)\n",
    "test_error = 1. - best_mlp_large.score(x_test, y_test)\n",
    "\n",
    "\n",
    "print ('\\nRESULTS FOR BEST NN\\n')\n",
    "\n",
    "print (f\"Best NN training error: {training_error:.4f}\")\n",
    "print (f\"Best NN test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e48d0569c46960a9e5dfccb5adbc3144",
     "grade": false,
     "grade_id": "cell-07de839aff93fd1f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TO DO 10\n",
    "\n",
    "Do you think data normalization is important? Why? Do you observe any difference between the results you obtained before and after scaling the data?\n",
    "\n",
    "(Answer in the next cell, no need to write code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78e3aa1fda8ad8a81a32c02232b8ab2e",
     "grade": true,
     "grade_id": "cell-eb534567f623d0dc",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Yes, data normalization is often important when working with machine learning algorithms. This is because many algorithms, especially those that use distance measures, assume that all features are on the same scale and have the same distribution. If this is not the case, then some features may dominate others and make it difficult for the algorithm to learn effectively.\n",
    "In my experience, data normalization can make a significant difference in the performance of machine learning algorithms. I have observed that algorithms often perform better and converge faster when the input data is normalized. However, the impact of normalization can vary depending on the specific algorithm and dataset, so it is always a good idea to try both normalized and non-normalized versions of the data to see which one works better"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "f1f40f8c330d568234d1a24f8987a48272071e5b522e677ff6ccdd68d8171358"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
